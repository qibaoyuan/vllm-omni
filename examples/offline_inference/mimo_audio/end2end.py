# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
"""
This example shows how to use vLLM for running offline inference
with the correct prompt format on MiMo-Audio-Omni.
"""

import json
import os
from typing import NamedTuple

import soundfile as sf
from message_convert import (
    get_audio_data,
    get_audio_understanding_sft_prompt,
    get_s2t_dialogue_sft_multiturn_prompt,
    get_spoken_dialogue_sft_multiturn_prompt,
    get_text_dialogue_sft_multiturn_prompt,
    get_tts_sft_prompt,
    to_prompt,
)
from vllm import SamplingParams
from vllm.utils.argparse_utils import FlexibleArgumentParser

from vllm_omni.entrypoints.omni import Omni
from vllm_omni.inputs.data import OmniTokensPrompt

SEED = 42
MAX_CODE2WAV_TOKENS = 18192


class QueryResult(NamedTuple):
    inputs: dict
    limit_mm_per_prompt: dict[str, int]


def get_codes_query_from_json(codes_path: str) -> QueryResult:
    with open(codes_path, encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, list):
        code_final = data
    elif isinstance(data, dict) and "code_final" in data:
        code_final = data["code_final"]
    else:
        raise ValueError(
            f"Unsupported codes json format in {codes_path}.\n"
            "Expect a JSON list[int] or {{'code_final': list[int]}}."
        )

    if not isinstance(code_final, list) or not all(isinstance(x, int) for x in code_final):
        raise ValueError("code_final must be a list[int].")

    if len(code_final) > MAX_CODE2WAV_TOKENS:
        print(f"[Warn] code_final len={len(code_final)} > {MAX_CODE2WAV_TOKENS}, truncating.")
        code_final = code_final[:MAX_CODE2WAV_TOKENS]

    return QueryResult(
        inputs=OmniTokensPrompt(
            prompt_token_ids=code_final,
            multi_modal_data=None,
            mm_processor_kwargs=None,
        ),
        limit_mm_per_prompt={},
    )


def get_tts_sft(
    text="ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    instruct=None,
    read_text_only=True,
    prompt_speech=None,
    audio_list=None,
):
    res = get_tts_sft_prompt(
        text,
        instruct=instruct,
        read_text_only=read_text_only,
        prompt_speech=prompt_speech,
    )

    prompt = to_prompt(res)
    final_prompt = {
        "prompt": prompt,
    }
    if audio_list is not None:
        final_prompt.update(
            {
                "multi_modal_data": {
                    "audio": audio_list,
                },
            }
        )
    return final_prompt


def get_audio_understanding_sft(audio_path, text="", thinking=False, use_sostm=False):
    audio_list = []
    audio_list.append(get_audio_data(audio_path))
    res = get_audio_understanding_sft_prompt(
        input_speech=audio_path, input_text=text, thinking=thinking, use_sostm=use_sostm
    )
    prompt = to_prompt(res)
    final_prompt = {
        "prompt": prompt,
        "multi_modal_data": {
            "audio": audio_list,
        },
    }
    return final_prompt


def get_spoken_dialogue_sft_multiturn(message_list, system_prompt=None, ref_audio_path=None, audio_list=None):
    res = get_spoken_dialogue_sft_multiturn_prompt(
        message_list, system_prompt=system_prompt, prompt_speech=ref_audio_path
    )
    prompt = to_prompt(res)
    final_prompt = {
        "prompt": prompt,
        "multi_modal_data": {
            "audio": audio_list,
        },
    }
    return final_prompt


def get_speech2text_dialogue_sft_multiturn(message_list, thinking=False, audio_list=None):
    res = get_s2t_dialogue_sft_multiturn_prompt(
        message_list,
        thinking=thinking,
    )
    prompt = to_prompt(res)
    final_prompt = {
        "prompt": prompt,
        "multi_modal_data": {
            "audio": audio_list,
        },
    }
    return final_prompt


def get_text_dialogue_sft_multiturn(
    message_list,
):
    res = get_text_dialogue_sft_multiturn_prompt(
        message_list,
    )
    prompt = to_prompt(res)
    final_prompt = {
        "prompt": prompt,
    }
    return final_prompt


query_map = {
    "tts_sft": get_tts_sft,
    "tts_sft_with_instruct": get_tts_sft,
    "tts_sft_with_audio": get_tts_sft,
    "tts_sft_with_natural_instruction": get_tts_sft,
    "audio_trancribing_sft": get_audio_understanding_sft,
    "audio_understanding_sft": get_audio_understanding_sft,
    "audio_understanding_sft_with_thinking": get_audio_understanding_sft,
    "spoken_dialogue_sft_multiturn": get_spoken_dialogue_sft_multiturn,
    "speech2text_dialogue_sft_multiturn": get_speech2text_dialogue_sft_multiturn,
    "text_dialogue_sft_multiturn": get_text_dialogue_sft_multiturn,
}


def main(args):
    model_name = args.model_name

    # Get paths from args
    text = getattr(args, "text", None)
    audio_path = getattr(args, "audio_path", None)

    instruct = getattr(args, "instruct", None)

    # Get the query function and call it with appropriate parameters
    query_func = query_map[args.query_type]

    omni_llm = Omni(
        model=model_name,
        stage_configs_path=args.stage_configs_path,
        log_stats=args.enable_stats,
        log_file=("omni_llm_pipeline.log" if args.enable_stats else None),
        init_sleep_seconds=args.init_sleep_seconds,
        batch_timeout=args.batch_timeout,
        init_timeout=args.init_timeout,
        shm_threshold_bytes=args.shm_threshold_bytes,
    )

    thinker_sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=50,
        max_tokens=1024,
        seed=SEED,
        logit_bias={},
        repetition_penalty=1.1,
    )

    code2wav_sampling_params = SamplingParams(
        temperature=0.0,
        top_p=1.0,
        top_k=-1,
        max_tokens=4096 * 16,
        seed=SEED,
        detokenize=True,
        repetition_penalty=1.1,
    )

    sampling_params_list = [
        thinker_sampling_params,
        code2wav_sampling_params,
    ]

    # Build query result based on query type
    # Notice: The audio files used in this example are available at: https://github.com/XiaomiMiMo/MiMo-Audio/tree/main/examples
    if args.query_type == "tts_sft":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type tts_sft
        """"
        lines ['Prompt:\n', '<|im_start|>user\nè¯·å°†è¿™æ®µæ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³: ä»Šå¤©å¤©æ°”çœŸå¥½<|im_end|>\n<|im_start|>assistant\n<|sostm|>\n', 'vllm_text_output:\n', 'ä»Šå¤©å¤©æ°”çœŸå¥½\n']
        Request ID: 0_f96f7bcd-a861-4fa0-a1f4-f804d8202be, Text saved to ./output_audio/tts_sft/0_f96f7bcd-a861-4fa0-a1f4-f804d8202be.txt
        Request ID: 0_f96f7bcd-a861-4fa0-a1f4-f804d8202be, Saved audio to ./output_audio/tts_sft/0_f96f7bcd-a861-4fa0-a1f4-f804d8202be.wav
        """
        query_result = query_func(text=text, read_text_only=True)
    elif args.query_type == "tts_sft_with_instruct":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type tts_sft_with_instruct --instruct "ç”¨å°å­©å­çš„å£°éŸ³å¼€å¿ƒçš„è¯´"
        """
        lines ['Prompt:\n', '<|im_start|>system\nYou need to generate speech based on the specified style instructions and text content.<|im_end|>\n<|im_start|>user\nè¯·å°†è¿™æ®µæ–‡å­—è½¬æ¢ä¸ºè¯­éŸ³: ä»Šå¤©å¤©æ°”çœŸå¥½(ç”¨å°å­©å­çš„å£°éŸ³å¼€å¿ƒçš„è¯´)<|im_end|>\n<|im_start|>assistant\n<think>\n\n', 'vllm_text_output:\n', 'å¥½çš„ï¼Œè¿™æ¬¡æ˜¯è¦æ¨¡ä»¿ä¸€ä¸ªå°å­©å­è¯´è¯ã€‚æŒ‡ä»¤å¾ˆæ˜ç¡®ï¼Œâ€œå°å­©å­â€ã€â€œå¼€å¿ƒâ€ã€‚é‚£æˆ‘çš„å£°éŸ³å°±è¦æå¾—é«˜ä¸€ç‚¹ï¼ŒéŸ³è‰²è¦äº®ä¸€äº›ï¼Œå¬èµ·æ¥å¤©çœŸæ— é‚ªã€‚è¯­é€Ÿå˜›ï¼Œä¸èƒ½å¤ªå¿«ï¼Œå¾—æœ‰ç‚¹æ…¢æ‚ æ‚ ã€ä¸€å­—ä¸€é¡¿çš„æ„Ÿè§‰ï¼Œå°±åƒå°æœ‹å‹åœ¨è®¤çœŸåœ°è¡¨è¾¾è‡ªå·±çš„å‘ç°ä¸€æ ·ã€‚â€œä»Šå¤©å¤©æ°”çœŸå¥½â€ï¼Œè¿™å¥è¯æœ¬èº«å°±æŒºé˜³å…‰çš„ï¼Œæ‰€ä»¥æˆ‘è¦å¸¦ç€é‚£ç§å‘è‡ªå†…å¿ƒçš„å–œæ‚¦æ„Ÿå»è¯´ï¼Œå¥å°¾å¯ä»¥ç¨å¾®ä¸Šæ‰¬ä¸€ç‚¹ç‚¹ï¼Œæ˜¾å¾—æ›´æ´»æ³¼å¯çˆ±ã€‚\n</think>\nä»Šå¤©å¤©æ°”çœŸå¥½\n']
        Request ID: 0_f6885005-c769-47ef-93fb-f22093fb42a6, Text saved to ./output_audio/tts_sft_with_instruct/0_f6885005-c769-47ef-93fb-f22093fb42a6.txt
        Request ID: 0_f6885005-c769-47ef-93fb-f22093fb42a6, Saved audio to ./output_audio/tts_sft_with_instruct/0_f6885005-c769-47ef-93fb-f22093fb42a6.wav
        """
        query_result = query_func(text=text, instruct=instruct, read_text_only=True)
    elif args.query_type == "tts_sft_with_audio":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type tts_sft_with_audio --audio_path "./spoken_dialogue_assistant_turn_1.wav"
        audio_list = [get_audio_data(audio_path)]
        query_result = query_func(text=text, read_text_only=True, prompt_speech=audio_path, audio_list=audio_list)
    elif args.query_type == "tts_sft_with_natural_instruction":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type tts_sft_with_natural_instruction --text "ç”¨æ°”å–˜ååçš„å¹´è½»ç”·æ€§å£°éŸ³è¯´ï¼šæˆ‘è·‘ä¸åŠ¨äº†ï¼Œä½ ç­‰ç­‰æˆ‘ï¼"
        """
        lines ['Prompt:\n', '<|im_start|>system\nYou need to generate speech based on the specified style instructions and text content.<|im_end|>\n<|im_start|>user\nç”¨æ°”å–˜ååçš„å¹´è½»ç”·æ€§å£°éŸ³è¯´ï¼šæˆ‘è·‘ä¸åŠ¨äº†ï¼Œä½ ç­‰ç­‰æˆ‘ï¼<|im_end|>\n<|im_start|>assistant\n<think>\n\n', 'vllm_text_output:\n', 'å¥½çš„ï¼Œè¿™ä¸ªè¦æ±‚å¾ˆæ˜ç¡®ã€‚é¦–å…ˆæ˜¯ä¸ªå¹´è½»ç”·æ€§çš„å£°éŸ³ï¼Œç„¶åå…³é”®æ˜¯â€œæ°”å–˜ååâ€ã€‚è¿™è¯´æ˜ä»–åˆšç»è¿‡å‰§çƒˆè¿åŠ¨ï¼Œä½“åŠ›ä¸æ”¯ã€‚æ‰€ä»¥æˆ‘çš„å£°éŸ³é‡Œå¾—å¸¦ä¸Šæ˜æ˜¾çš„å–˜æ¯å£°ï¼Œå°¤å…¶æ˜¯åœ¨å¥å­çš„å¼€å¤´å’Œç»“å°¾ã€‚è¯­é€Ÿè¦æ”¾æ…¢ï¼Œæ–­æ–­ç»­ç»­çš„ï¼Œå¥½åƒæ¯è¯´ä¸€ä¸ªå­—éƒ½å¾ˆè´¹åŠ²ã€‚â€œæˆ‘è·‘ä¸åŠ¨äº†â€è¿™é‡Œå¯ä»¥è¡¨ç°å‡ºä¸€ç§æ— åŠ›æ„Ÿï¼ŒéŸ³è°ƒç¨å¾®æœ‰ç‚¹ä¸Šæ‰¬ä½†åˆå¾ˆå¿«è½ä¸‹å»ã€‚åˆ°äº†â€œä½ ç­‰ç­‰æˆ‘ï¼â€çš„æ—¶å€™ï¼Œæƒ…ç»ªè¦æ›´æ€¥åˆ‡ä¸€ç‚¹ï¼Œå› ä¸ºæ˜¯åœ¨æ±‚äººï¼Œä½†èº«ä½“çŠ¶æ€è¿˜æ˜¯è·Ÿä¸ä¸Šï¼Œæ‰€ä»¥è¿™ç§æ€¥åˆ‡æ˜¯è™šå¼±ä¸­çš„æ€¥åˆ‡ã€‚å—¯ï¼Œé‡ç‚¹å°±æ˜¯æŠŠé‚£ç§ä¸Šæ°”ä¸æ¥ä¸‹æ°”çš„æ„Ÿè§‰ç»™åšå‡ºæ¥ã€‚\n</think>\næˆ‘è·‘ä¸åŠ¨äº†ï¼Œä½ ç­‰ç­‰æˆ‘ï¼\n']
        Request ID: 0_7c161be3-96d3-46b1-9981-a59fa1ae81e5, Text saved to ./output_audio/tts_sft_with_natural_instruction/0_7c161be3-96d3-46b1-9981-a59fa1ae81e5.txt
        Request ID: 0_7c161be3-96d3-46b1-9981-a59fa1ae81e5, Saved audio to ./output_audio/tts_sft_with_natural_instruction/0_7c161be3-96d3-46b1-9981-a59fa1ae81e5.wav        """
        query_result = query_func(text=text, read_text_only=False)
    elif args.query_type == "audio_trancribing_sft":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type audio_trancribing_sft --audio_path "./spoken_dialogue_assistant_turn_1.wav"
        """
        lines ['Prompt:\n', '<|im_start|>user\n<|sosp|><|empty|><|eosp|>Please transcribe this audio and repeat it once.<|im_end|>\n<|im_start|>assistant\n<|sostm|>\n', 'vllm_text_output:\n', 'ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ\n']
        Request ID: 0_a9c107ec-7a4e-44fe-a304-d3ee6e1dcca6, Text saved to ./output_audio/audio_trancribe_sft/0_a9c107ec-7a4e-44fe-a304-d3ee6e1dcca6.txt
        Request ID: 0_a9c107ec-7a4e-44fe-a304-d3ee6e1dcca6, Audio saved to ./output_audio/audio_trancribe_sft/0_a9c107ec-7a4e-44fe-a304-d3ee6e1dcca6.wav
        """
        audio_path = "spoken_dialogue_assistant_turn_1.wav"
        text = "Please transcribe this audio and repeat it once."
        query_result = query_func(text=text, audio_path=audio_path, use_sostm=True)
    elif args.query_type == "audio_understanding_sft":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type audio_understanding_sft --text "Summarize the audio." --audio_path "./spoken_dialogue_assistant_turn_1.wav"
        """
        lines ['Prompt:\n', '<|im_start|>user\n<|sosp|><|empty|><|eosp|>Summarize the audio.<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n', 'vllm_text_output:\n', "The speaker provides several ways to check today's weather, including using built-in phone features (like Apple Weather), professional services (such as AccuWeather or China Meteoweb), and search engines (Google or Baidu). They also mention that while they can analyze historical weather trends for a specific city, real-time data must be obtained through official sources. The speaker invites the listener to share their location for further assistance.\n"]
        Request ID: 0_0e3dd143-99fd-4f37-8d0c-f78859e76665, Text saved to ./output_audio/audio_understanding_sft/0_0e3dd143-99fd-4f37-8d0c-f78859e76665.txt
        """
        query_result = query_func(text=text, audio_path=audio_path)
    elif args.query_type == "audio_understanding_sft_with_thinking":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type audio_understanding_sft_with_thinking --text "Summarize the audio." --audio_path "./spoken_dialogue_assistant_turn_1.wav"
        """
        lines ['Prompt:\n', '<|im_start|>user\n<|sosp|><|empty|><|eosp|>Summarize the audio.<|im_end|>\n<|im_start|>assistant\n<think>\n\n', 'vllm_text_output:\n', 'The user wants a summary of the provided audio transcript.\n\n1.  **Identify the core topic:** The main subject is how to check today\'s weather forecast.\n2.  **Recognize the key constraint:** The speaker explicitly states they cannot access real-time data themselves ("æˆ‘æ²¡åŠæ³•è·å–å®æ—¶çš„å¤©æ°”ä¿¡æ¯").\n3.  **List the methods suggested:** The speaker provides several alternative ways for the listener to find the weather information:\n    *   Using built-in phone features (specifically mentioning Apple\'s Weather app and checking in "ç³»ç»Ÿè®¾ç½®" - system settings).\n    *   Using professional weather services, giving examples like AccuWeather, Weather.com, and Chinese services like ä¸­æœ€å¤©æ°”ç½‘ (zhongzuiweather.com) and æ¢…èŠ±å¤©æ°” (mehua weather).\n    *   Using search engines (Google or Baidu) by searching for "[city name] + å¤©æ°”" ([åŸå¸‚å] + weather).\n4.  **Note any additional offers or conditions:** The speaker offers to help analyze historical weather trends if the listener provides their city name, but reiterates that current data must be obtained from official sources.\n5.  **Synthesize into a concise summary:** Combine these points into a clear and brief paragraph. Start with the main point (the inability to get live data), then list the recommended methods, and finally include the offer for historical analysis. This structure accurately reflects the content and flow of the original audio.\n</think>\nThe speaker explains that they cannot provide real-time weather information directly. Instead, they suggest several methods for the listener to check the current weather:\n\n*   Use the built-in weather application on your smartphone (like Apple\'s Weather app).\n*   Visit professional weather websites such as AccuWeather, Weather.com, ä¸­æœ€å¤©æ°”ç½‘, or æ¢…èŠ±å¤©æ°”.\n*   Search for your city followed by the word "å¤©æ°”" (weather) using Google or Baidu.\n\nThe speaker also offers to help analyze historical weather trends for a specific city if the listener provides its name, but emphasizes that all current data should be obtained through official channels.\n']
        Request ID: 0_7899d15a-1d5c-439a-9888-dd8c807b8165, Text saved to ./output_audio/audio_understanding_sft_with_thinking/0_7899d15a-1d5c-439a-9888-dd8c807b8165.txt
        """
        query_result = query_func(text=text, audio_path=audio_path, thinking=True)
    elif args.query_type == "spoken_dialogue_sft_multiturn":
        # python3 -u end2end.py --stage-configs-path ${config_file} --model ${MODEL_PATH}  --query-type spoken_dialogue_sft_multiturn  --audio_path "./prompt_speech_zh_m.wav"
        """
        lines ['Prompt:\n', '<|im_start|>system\nYour Voice Should be:<|sosp|><|empty|><|eosp|><|im_end|>\n<|im_start|>user\n<|sosp|><|empty|><|eosp|><|im_end|>\n<|im_start|>assistant\næˆ‘æ²¡åŠæ³•è·å–å®æ—¶çš„å¤©æ°”ä¿¡æ¯ã€‚ä¸è¿‡å‘¢ï¼Œä½ å¯ä»¥è¯•è¯•å‡ ä¸ªæ–¹æ³•æ¥æŸ¥çœ‹ä»Šå¤©çš„å¤©æ°”ã€‚é¦–å…ˆï¼Œä½ å¯ä»¥ç”¨æ‰‹æœºè‡ªå¸¦çš„å¤©æ°”åŠŸèƒ½ï¼Œæ¯”å¦‚è‹¹æœæ‰‹æœºçš„å¤©æ°”åº”ç”¨ï¼Œæˆ–è€…ç›´æ¥åœ¨ç³»ç»Ÿè®¾ç½®é‡ŒæŸ¥çœ‹ã€‚å…¶æ¬¡ï¼Œä½ ä¹Ÿå¯ä»¥ç”¨ä¸€äº›ä¸“ä¸šçš„å¤©æ°”æœåŠ¡ï¼Œåƒæ˜¯å›½å¤–çš„AccuWeatherã€Weather.comï¼Œæˆ–è€…å›½å†…çš„ä¸­å›½å¤©æ°”ç½‘ã€å¢¨è¿¹å¤©æ°”ç­‰ç­‰ã€‚å†æœ‰å°±æ˜¯ï¼Œä½ è¿˜å¯ä»¥åœ¨è°·æ­Œæˆ–è€…ç™¾åº¦é‡Œç›´æ¥æœç´¢ä½ æ‰€åœ¨çš„åŸå¸‚åŠ ä¸Šå¤©æ°”è¿™ä¸¤ä¸ªå­—ã€‚å¦‚æœä½ èƒ½å‘Šè¯‰æˆ‘ä½ æ‰€åœ¨çš„åŸå¸‚ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åˆ†æä¸€ä¸‹å†å²å¤©æ°”è¶‹åŠ¿ï¼Œä¸è¿‡æœ€æ–°çš„æ•°æ®è¿˜æ˜¯éœ€è¦ä½ é€šè¿‡å®˜æ–¹æ¸ é“å»è·å–å“¦ã€‚<|sosp|><|empty|><|eosp|><|im_end|>\n<|im_start|>user\n<|sosp|><|empty|><|eosp|><|im_end|>\n<|im_start|>assistant\n<|sostm|>\n', 'vllm_text_output:\n', 'å¥½çš„ï¼Œä¸ºæ‚¨æŸ¥è¯¢åˆ°åŒ—äº¬å½“å‰çš„å¤©æ°”æƒ…å†µæ˜¯è¿™æ ·çš„ï¼šé¦–å…ˆæ˜¯æ¸©åº¦ï¼Œç°åœ¨æ˜¯é›¶ä¸‹3æ‘„æ°åº¦ï¼Œä½“æ„Ÿéå¸¸å¯’å†·ã€‚å¤©æ°”çŠ¶å†µæ˜¯æ™´å¤©ï¼Œæ¹¿åº¦ç™¾åˆ†ä¹‹å››åäº”ï¼Œç©ºæ°”è´¨é‡æŒ‡æ•°æ˜¯120ï¼Œå±äºè½»åº¦æ±¡æŸ“ï¼Œä¸»è¦æ±¡æŸ“ç‰©æ˜¯PM2.5ã€‚é£å‘æ˜¯è¥¿åŒ—é£ï¼Œé£åŠ›ä¸å¤§ï¼Œåœ¨æ¯ç§’2åˆ°4ç±³ä¹‹é—´ã€‚æ°”å‹æ˜¯ä¸€åƒé›¶äºŒåäºŒç™¾å¸•ã€‚ä»Šå¤©ç™½å¤©çš„æœ€é«˜æ°”æ¸©æ˜¯é›¶ä¸Š2æ‘„æ°åº¦ï¼Œå¤œé—´æœ€ä½æ°”æ¸©ä¼šé™åˆ°é›¶ä¸‹6æ‘„æ°åº¦ã€‚å¦å¤–è¿˜æœ‰ä¸¤ä¸ªå°è´´å£«ç»™æ‚¨ï¼šç¬¬ä¸€ï¼Œå› ä¸ºæ¸©å·®å¤§ï¼Œè¯·æ³¨æ„é˜²å¯’ä¿æš–ï¼Œç‰¹åˆ«æ˜¯è¦ä¿æŠ¤å¥½è€³æœµå’Œæ‰‹æŒ‡è¿™äº›éœ²åœ¨å¤–é¢çš„çš®è‚¤ã€‚ç¬¬äºŒï¼Œç›®å‰ç©ºæ°”è´¨é‡ä¸å¤ªå¥½ï¼Œå»ºè®®æ‚¨å‡å°‘æˆ·å¤–æ´»åŠ¨çš„æ—¶é—´ï¼Œå¦‚æœéœ€è¦ç”¨å£ç½©çš„è¯ï¼Œæœ€å¥½é€‰æ‹©N95çº§åˆ«çš„ã€‚å¦‚æœæ‚¨æƒ³æŸ¥å…¶ä»–åŸå¸‚æˆ–è€…æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘å…·ä½“çš„åŸå¸‚åæˆ–è€…æ—¥æœŸï¼Œæˆ‘ä¼šå¸®æ‚¨è°ƒæ•´çš„ï¼\n']
        Request ID: 0_a2b4a232-2b86-442f-8fbb-d9b8fd198b00, Text saved to ./output_audio/spoken_dialogue_sft_multiturn/0_a2b4a232-2b86-442f-8fbb-d9b8fd198b00.txt
        Request ID: 0_a2b4a232-2b86-442f-8fbb-d9b8fd198b00, Saved audio to ./output_audio/spoken_dialogue_sft_multiturn/0_a2b4a232-2b86-442f-8fbb-d9b8fd198b00.wav
        """
        first_turn_text_response = "æˆ‘æ²¡åŠæ³•è·å–å®æ—¶çš„å¤©æ°”ä¿¡æ¯ã€‚ä¸è¿‡å‘¢ï¼Œä½ å¯ä»¥è¯•è¯•å‡ ä¸ªæ–¹æ³•æ¥æŸ¥çœ‹ä»Šå¤©çš„å¤©æ°”ã€‚é¦–å…ˆï¼Œä½ å¯ä»¥ç”¨æ‰‹æœºè‡ªå¸¦çš„å¤©æ°”åŠŸèƒ½ï¼Œæ¯”å¦‚è‹¹æœæ‰‹æœºçš„å¤©æ°”åº”ç”¨ï¼Œæˆ–è€…ç›´æ¥åœ¨ç³»ç»Ÿè®¾ç½®é‡ŒæŸ¥çœ‹ã€‚å…¶æ¬¡ï¼Œä½ ä¹Ÿå¯ä»¥ç”¨ä¸€äº›ä¸“ä¸šçš„å¤©æ°”æœåŠ¡ï¼Œåƒæ˜¯å›½å¤–çš„AccuWeatherã€Weather.comï¼Œæˆ–è€…å›½å†…çš„ä¸­å›½å¤©æ°”ç½‘ã€å¢¨è¿¹å¤©æ°”ç­‰ç­‰ã€‚å†æœ‰å°±æ˜¯ï¼Œä½ è¿˜å¯ä»¥åœ¨è°·æ­Œæˆ–è€…ç™¾åº¦é‡Œç›´æ¥æœç´¢ä½ æ‰€åœ¨çš„åŸå¸‚åŠ ä¸Šå¤©æ°”è¿™ä¸¤ä¸ªå­—ã€‚å¦‚æœä½ èƒ½å‘Šè¯‰æˆ‘ä½ æ‰€åœ¨çš„åŸå¸‚ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åˆ†æä¸€ä¸‹å†å²å¤©æ°”è¶‹åŠ¿ï¼Œä¸è¿‡æœ€æ–°çš„æ•°æ®è¿˜æ˜¯éœ€è¦ä½ é€šè¿‡å®˜æ–¹æ¸ é“å»è·å–å“¦ã€‚"
        audio_list = []
        s1_audio_path = "ä»Šå¤©å¤©æ°”å¦‚ä½•.mp3"
        s2_audio_path = "spoken_dialogue_assistant_turn_1.wav"
        s3_audio_path = "åŒ—äº¬.mp3"
        audio_list.append(get_audio_data(audio_path))
        audio_list.append(get_audio_data(s1_audio_path))
        audio_list.append(get_audio_data(s2_audio_path))
        audio_list.append(get_audio_data(s3_audio_path))

        message_list = [
            {"role": "user", "content": s1_audio_path},
            {"role": "assistant", "content": {"text": first_turn_text_response, "audio": s2_audio_path}},
            {"role": "user", "content": s3_audio_path},
        ]
        query_result = query_func(message_list, system_prompt=None, ref_audio_path=audio_path, audio_list=audio_list)
    elif args.query_type == "speech2text_dialogue_sft_multiturn":
        # python3 -u end2end.py --stage-configs-path ${config_file_only_llm} --model ${MODEL_PATH}  --query-type speech2text_dialogue_sft_multiturn
        """
        lines ['Prompt:\n', '<|im_start|>user\n<|sosp|><|empty|><|eosp|><|im_end|>\n<|im_start|>assistant\nä½ å¥½ï¼Œæˆ‘æ²¡åŠæ³•è·å–å®æ—¶çš„å¤©æ°”ä¿¡æ¯ã€‚å¦‚æœä½ èƒ½å‘Šè¯‰æˆ‘ä½ æ‰€åœ¨çš„åŸå¸‚ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åˆ†æä¸€ä¸‹å†å²å¤©æ°”è¶‹åŠ¿ï¼Œä¸è¿‡æœ€æ–°çš„æ•°æ®è¿˜æ˜¯éœ€è¦ä½ é€šè¿‡å®˜æ–¹æ¸ é“å»è·å–å“¦ã€‚<|im_end|>\n<|im_start|>user\n<|sosp|><|empty|><|eosp|><|im_end|>\n<|im_start|>assistant\n<think>\n\n', 'vllm_text_output:\n', 'å¥½çš„ï¼Œç”¨æˆ·é—®çš„æ˜¯åŒ—äº¬çš„å¤©æ°”æƒ…å†µã€‚è™½ç„¶æˆ‘æ— æ³•æä¾›å®æ—¶æ•°æ®ï¼Œä½†æˆ‘å¯ä»¥å‘Šè¯‰ç”¨æˆ·æŸ¥è¯¢å¤©æ°”çš„å¯é é€”å¾„ï¼Œå¹¶æ ¹æ®å†å²è§„å¾‹ç»™å‡ºä¸€äº›å‚è€ƒå»ºè®®ã€‚\n\né¦–å…ˆï¼Œæœ€å‡†ç¡®çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸“ä¸šçš„å¤©æ°”åº”ç”¨æˆ–ç½‘ç«™ï¼Œæ¯”å¦‚ä¸­å›½æ°”è±¡å±€å®˜ç½‘ã€ä¸­å¤®ç”µè§†å°å¤©æ°”é¢„æŠ¥æˆ–è€…åƒå¢¨è¿¹å¤©æ°”è¿™æ ·çš„æ‰‹æœºåº”ç”¨ã€‚è¿™äº›å¹³å°çš„æ•°æ®éƒ½æ¥è‡ªæƒå¨æœºæ„ï¼Œæ›´æ–°åŠæ—¶ï¼Œè¿˜èƒ½çœ‹åˆ°å…·ä½“çš„æ¸©åº¦ã€æ¹¿åº¦ã€é£é€Ÿå’Œç©ºæ°”è´¨é‡æŒ‡æ•°ç­‰è¯¦ç»†ä¿¡æ¯ã€‚\n\nå…¶æ¬¡ï¼Œå¦‚æœç”¨æˆ·æƒ³äº†è§£é•¿æœŸè¶‹åŠ¿ï¼Œæˆ‘å¯ä»¥åˆ†äº«ä¸€äº›åŒ—äº¬çš„å†å²æ°”å€™ç‰¹ç‚¹ã€‚åŒ—äº¬å±äºæ¸©å¸¦å­£é£æ°”å€™ï¼Œå››å­£åˆ†æ˜ã€‚æ˜¥å­£é€šå¸¸åœ¨3æœˆåˆ°5æœˆï¼Œç‰¹ç‚¹æ˜¯å¹²ç‡¥å¤šé£ï¼Œå¶å°”æœ‰æ²™å°˜å¤©æ°”ï¼Œå¹³å‡æ°”æ¸©ä»10æ‘„æ°åº¦å·¦å³é€æ¸å‡åˆ°25æ‘„æ°åº¦ä»¥ä¸Šã€‚å¤å­£æ˜¯6æœˆåˆ°8æœˆï¼Œç‚çƒ­å¤šé›¨ï¼Œå¹³å‡æ°”æ¸©åœ¨25åˆ°30æ‘„æ°åº¦ä¹‹é—´ï¼Œ7æœˆä»½æœ€çƒ­çš„æ—¶å€™å¯èƒ½è¾¾åˆ°35æ‘„æ°åº¦ä»¥ä¸Šï¼Œè€Œä¸”ç»å¸¸æœ‰é›·é˜µé›¨ã€‚ç§‹å­£æ˜¯ä»9æœˆåˆ°11æœˆï¼Œå¤©æ°”å‡‰çˆ½å®œäººï¼Œå¹³å‡æ°”æ¸©åœ¨15åˆ°25æ‘„æ°åº¦ï¼Œæ˜¯æ—…æ¸¸çš„å¥½å­£èŠ‚ã€‚å†¬å­£åˆ™æ˜¯åœ¨12æœˆåˆ°æ¬¡å¹´2æœˆï¼Œå¯’å†·å¹²ç‡¥ï¼Œå¹³å‡æ°”æ¸©åœ¨é›¶ä¸‹5æ‘„æ°åº¦åˆ°5æ‘„æ°åº¦ä¹‹é—´ï¼Œ1æœˆä»½æœ€å†·æ—¶å¯èƒ½ä½è‡³é›¶ä¸‹15æ‘„æ°åº¦ï¼Œé™é›ªä¸å¤šä½†é£å¯’æ•ˆåº”æ˜æ˜¾ã€‚\n\nå¦å¤–ï¼Œæˆ‘è¿˜å¾—æé†’ç”¨æˆ·æ³¨æ„ç©ºæ°”è´¨é‡ã€‚åŒ—äº¬çš„PM2.5æŒ‡æ•°æœ‰æ—¶ä¼šæ¯”è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å†¬å¤©ä¾›æš–æœŸé—´ï¼Œå»ºè®®å…³æ³¨AQIæŒ‡æ•°ï¼Œå¿…è¦æ—¶ä½©æˆ´å£ç½©ã€‚ç©¿è¡£æ–¹é¢ä¹Ÿè¦æ ¹æ®å®æ—¶å¤©æ°”è°ƒæ•´ï¼Œå¤å¤©é˜²æ™’é˜²é›¨ï¼Œå†¬å¤©ä¿æš–é˜²é£ã€‚\n\næœ€åï¼Œå¦‚æœç”¨æˆ·éœ€è¦æ›´å…·ä½“çš„ä¿¡æ¯ï¼Œæ¯”å¦‚æœªæ¥ä¸€å‘¨çš„é¢„æŠ¥æˆ–è€…æŸä¸ªç‰¹å®šæ—¥æœŸçš„å¤©æ°”ï¼Œæœ€å¥½è¿˜æ˜¯é€šè¿‡ä¸Šè¿°ä¸“ä¸šæ¸ é“æŸ¥è¯¢ï¼Œè¿™æ ·å¾—åˆ°çš„ç»“æœæ‰æœ€å‡†ç¡®å¯é ã€‚\n</think>\nå…³äºåŒ—äº¬å½“å‰çš„å¤©æ°”ï¼Œç”±äºæˆ‘æ— æ³•è®¿é—®å®æ—¶æ•°æ®ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å®ç”¨å»ºè®®ï¼š\n\n### 1ï¸âƒ£ **æ¨èæŸ¥è¯¢æ–¹å¼**\n- **å®˜æ–¹æ¸ é“**ï¼š  \n  - ä¸­å›½æ°”è±¡å±€å®˜ç½‘ï¼ˆ[www.nmc.cn](http://www.nmc.cn)ï¼‰  \n  - å¤®è§†ã€Šæ–°é—»è”æ’­ã€‹åçš„å¤©æ°”é¢„æŠ¥ï¼ˆçº¦æ™šä¸Š7ç‚¹ï¼‰\n- **å¸¸ç”¨APP**ï¼š  \n  å¢¨è¿¹å¤©æ°”ã€å½©äº‘å¤©æ°”ã€AccuWeatherï¼ˆå¯æŸ¥çœ‹æ¯å°æ—¶é™æ°´æ¦‚ç‡ï¼‰\n\n### 2ï¸âƒ£ **è¿‘æœŸå…¸å‹å¤©æ°”ç‰¹å¾**\n- **æ˜¥ç§‹å­£**ï¼ˆ3-5æœˆ/9-11æœˆï¼‰ï¼š  \n  æ˜¼å¤œæ¸©å·®å¤§ï¼ˆÂ±10â„ƒï¼‰ï¼Œéœ€å¤‡å¤–å¥—  \n- **å¤å­£**ï¼ˆ6-8æœˆï¼‰ï¼š  \n  é«˜æ¸©å¸¸è¾¾30â„ƒ+ï¼Œåˆåå±€éƒ¨é™é›¨  \n- **å†¬å­£**ï¼ˆ12-2æœˆï¼‰ï¼š  \n  å¹³å‡ä½æ¸©-5â„ƒï¼Œé›¾éœ¾é«˜å‘æœŸ\n\n### 3ï¸âƒ£ **å‡ºè¡Œå°è´´å£«**\n- æŸ¥çœ‹å®æ—¶äº¤é€šè·¯å†µï¼ˆç™¾åº¦åœ°å›¾/é«˜å¾·ï¼‰  \n- æå‰å…³æ³¨ç©ºæ°”è´¨é‡æŒ‡æ•°ï¼ˆAQIï¼150å»ºè®®å‡å°‘æˆ·å¤–æ´»åŠ¨ï¼‰  \n- è‹¥è®¡åˆ’çˆ¬å±±ï¼ˆå¦‚é¦™å±±ï¼‰ï¼Œè¯·ç¡®è®¤å½“æ—¥æ˜¯å¦å°è·¯\n\nå»ºè®®æ‚¨é€šè¿‡ä¸Šè¿°ä»»ä¸€æ¸ é“å¿«é€Ÿè·å–æœ€æ–°ä¿¡æ¯ã€‚å¦‚éœ€å…¶ä»–å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘ŠçŸ¥ï¼ ğŸŒ¦ï¸\n']
        """
        sampling_params_list = [
            thinker_sampling_params,
        ]
        s1_audio_path = "ä»Šå¤©å¤©æ°”å¦‚ä½•.mp3"
        s2_audio_path = "åŒ—äº¬.mp3"
        audio_list = []
        audio_list.append(get_audio_data(s1_audio_path))
        audio_list.append(get_audio_data(s2_audio_path))
        message_list = [
            {"role": "user", "content": s1_audio_path},
            {
                "role": "assistant",
                "content": "ä½ å¥½ï¼Œæˆ‘æ²¡åŠæ³•è·å–å®æ—¶çš„å¤©æ°”ä¿¡æ¯ã€‚å¦‚æœä½ èƒ½å‘Šè¯‰æˆ‘ä½ æ‰€åœ¨çš„åŸå¸‚ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åˆ†æä¸€ä¸‹å†å²å¤©æ°”è¶‹åŠ¿ï¼Œä¸è¿‡æœ€æ–°çš„æ•°æ®è¿˜æ˜¯éœ€è¦ä½ é€šè¿‡å®˜æ–¹æ¸ é“å»è·å–å“¦ã€‚",
            },
            {"role": "user", "content": s2_audio_path},
        ]
        query_result = query_func(message_list, thinking=True, audio_list=audio_list)
    elif args.query_type == "text_dialogue_sft_multiturn":
        # python3 -u end2end.py --stage-configs-path ${config_file_only_llm} --model ${MODEL_PATH}  --query-type text_dialogue_sft_multiturn
        """
        lines ['Prompt:\n', '<|im_start|>user\nå¯ä»¥ç»™æˆ‘ä»‹ç»ä¸€äº›ä¸­å›½çš„æ—…æ¸¸æ™¯ç‚¹å—ï¼Ÿ<|im_end|>\n<|im_start|>assistant\nä½ å¥½ï¼Œæ‚¨æƒ³å»å“ªä¸ªåŸå¸‚æ—…æ¸¸å‘¢ï¼Ÿ<|im_end|>\n<|im_start|>user\nåŒ—äº¬<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n', 'vllm_text_output:\n', 'å½“ç„¶ï¼åŒ—äº¬ä½œä¸ºä¸­å›½é¦–éƒ½ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„å†å²æ–‡åŒ–å’Œç°ä»£æ™¯è§‚ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å€¼å¾—ä¸€æ¸¸çš„æ™¯ç‚¹æ¨èï¼š\n\n---\n\n### **1. æ•…å®«ï¼ˆç´«ç¦åŸï¼‰**\n- **ç‰¹è‰²**ï¼šæ˜æ¸…ä¸¤ä»£çš‡å®¶å®«æ®¿ï¼Œä¸–ç•Œæœ€å¤§ã€ä¿å­˜æœ€å®Œæ•´çš„æœ¨è´¨ç»“æ„å¤å»ºç­‘ç¾¤ã€‚\n- **äº®ç‚¹**ï¼šå¤ªå’Œæ®¿ã€çå®é¦†ã€é’Ÿè¡¨é¦†ï¼›å†¬å­£å¯ä½“éªŒâ€œæ•…å®«é›ªæ™¯â€ã€‚\n- **é—¨ç¥¨**ï¼šæ—ºå­£60å…ƒ/äººï¼Œéœ€æå‰é¢„çº¦ã€‚\n\n### **2. é•¿åŸï¼ˆå…«è¾¾å²­/æ…•ç”°å³ªï¼‰**\n- **å…«è¾¾å²­é•¿åŸ**ï¼šæœ€ç»å…¸æ®µè½ï¼Œäº¤é€šä¾¿åˆ©ï¼Œé€‚åˆåˆæ¬¡æ¸¸è§ˆã€‚\n- **æ…•ç”°å³ªé•¿åŸ**ï¼šé£æ™¯ç§€ä¸½ï¼Œäººç›¸å¯¹è¾ƒå°‘ï¼Œé€‚åˆæ‹ç…§ã€‚\n- **å»ºè®®**ï¼šæ¸…æ™¨æˆ–å‚æ™šæ¸¸è§ˆé¿å¼€äººæµï¼Œç©¿èˆ’é€‚è¿åŠ¨é‹ã€‚\n\n### **3. å¤©å®‰é—¨å¹¿åœº & å›½å®¶åšç‰©é¦†**\n- **å¤©å®‰é—¨å¹¿åœº**ï¼šä¸–ç•Œä¸Šæœ€å¤§çš„åŸå¸‚å¹¿åœºï¼Œå¯çœ‹å‡æ——ä»ªå¼ï¼ˆéœ€æŸ¥æ—¶é—´è¡¨ï¼‰ã€‚\n- **å›½å®¶åšç‰©é¦†**ï¼šå…è´¹å¼€æ”¾ï¼Œå±•ç¤ºä¸­åäº”åƒå¹´æ–‡æ˜ã€‚\n\n### **4. é¢å’Œå›­**\n- **ç‰¹è‰²**ï¼šæ¸…ä»£çš‡å®¶å›­æ—ï¼Œä»¥æ˜†æ˜æ¹–ã€ä¸‡å¯¿å±±ä¸ºåŸºå€ï¼Œèåˆæ±Ÿå—å›­æ—é£æ ¼ã€‚\n- **å¿…çœ‹**ï¼šé•¿å»Šå½©ç»˜ã€ä½›é¦™é˜ã€åä¸ƒå­”æ¡¥ã€‚\n\n### **5. åŒ—äº¬èƒ¡åŒä¸å››åˆé™¢**\n- **æ¨èåŒºåŸŸ**ï¼š\n  - **å—é”£é¼“å··**ï¼šæ–‡è‰ºå°åº—èšé›†åœ°ï¼Œé€‚åˆå¹´è½»äººæ‰“å¡ã€‚\n  - **ä»€åˆ¹æµ·**ï¼šåæµ·é…’å§è¡—å¤œç”Ÿæ´»ï¼Œåˆ’èˆ¹èµç§‹å¶ã€‚\n  - **æ¨æ¢…ç«¹æ–œè¡—**ï¼šå°ä¼—èƒ¡åŒï¼Œå’–å•¡é¦†ä¸æ–‡åˆ›åº—ã€‚\n\n### **6. æ™¯å±±å…¬å›­**\n- **ç™»é¡¶ä¿¯ç°**ï¼šæ•…å®«å…¨æ™¯æœ€ä½³è§‚æ™¯ç‚¹ï¼Œæ—¥è½æ—¶åˆ†å°¤å…¶ç¾ã€‚\n\n### **7. å¥¥æ—åŒ¹å…‹å…¬å›­ï¼ˆé¸Ÿå·¢ã€æ°´ç«‹æ–¹ï¼‰**\n- **ç°ä»£åœ°æ ‡**ï¼š2008å¹´å¥¥è¿ä¼šåœºé¦†ï¼Œå¤œæ™šç¯å…‰ç§€å¾ˆéœ‡æ’¼ã€‚\n\n### **8. è¥¿çº¢é—¨é‡ç”ŸåŠ¨ç‰©å›­**\n- **äº²å­æ¸¸é¦–é€‰**ï¼šå¯è‡ªé©¾æˆ–ä¹˜å°ç«è½¦è¿‘è·ç¦»æ¥è§¦åŠ¨ç‰©ã€‚\n\n### **9. ç‰æ¸Šæ½­å…¬å›­**\n- **æ˜¥å­£æ¨±èŠ±**ï¼š3æœˆåº•è‡³4æœˆåˆæ¨±èŠ±ç››å¼€ï¼Œæ˜¯çƒ­é—¨èµæ¨±åœ°ã€‚\n\n### **10. åœ°é“é‡Œçš„æ–‡åŒ–ç«™**\n- **æ¨èç«™ç‚¹**ï¼šä¸œåé—¨ï¼ˆå¤ä»£çš‡åŸï¼‰ã€é¼“æ¥¼å¤§è¡—ï¼ˆè€åŒ—äº¬é£æƒ…ï¼‰ã€è¥¿ç›´é—¨ï¼ˆäº¤é€šæ¢çº½ï¼‰ã€‚\n\n---\n\n### **æ—…è¡Œå°è´´å£«**\n- **äº¤é€š**ï¼šåœ°é“è¦†ç›–å¹¿ï¼Œä¸‹è½½â€œäº¿é€šè¡Œâ€APPæ‰«ç ä¹˜è½¦ï¼›å…±äº«å•è½¦æ–¹ä¾¿çŸ­é€”ã€‚\n- **ç¾é£Ÿ**ï¼šçƒ¤é¸­ï¼ˆå››å­£æ°‘ç¦ã€å¤§è‘£ï¼‰ã€ç‚¸é…±é¢ã€è±†æ±å„¿ï¼ˆå°è¯•å‰åšå¥½å¿ƒç†å‡†å¤‡ï¼‰ã€‚\n- **å­£èŠ‚**ï¼šæ˜¥ç§‹æœ€ä½³ï¼ˆ3-5æœˆã€9-11æœˆï¼‰ï¼Œå¤å­£ç‚çƒ­ï¼Œå†¬å­£å¯’å†·ä½†å¯æ»‘é›ªï¼ˆå¦‚å—å±±æ»‘é›ªåœºï¼‰ã€‚\n\nå¦‚æœéœ€è¦æ›´å…·ä½“çš„è·¯çº¿è§„åˆ’æˆ–æ·±åº¦ä½“éªŒå»ºè®®ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ä½ çš„å…´è¶£åå¥½å“¦ï¼ ğŸ˜Š\n']
        Request ID: 0_32f2ec15-accc-4d78-bfe0-c61788e56299, Text saved to ./output_audio/text_dialogue_sft_multiturn/0_32f2ec15-accc-4d78-bfe0-c61788e56299.txt
        """
        sampling_params_list = [
            thinker_sampling_params,
        ]
        message_list = [
            {"role": "user", "content": "å¯ä»¥ç»™æˆ‘ä»‹ç»ä¸€äº›ä¸­å›½çš„æ—…æ¸¸æ™¯ç‚¹å—ï¼Ÿ"},
            {"role": "assistant", "content": "ä½ å¥½ï¼Œæ‚¨æƒ³å»å“ªä¸ªåŸå¸‚æ—…æ¸¸å‘¢ï¼Ÿ"},
            {"role": "user", "content": "åŒ—äº¬"},
        ]
        query_result = query_func(message_list=message_list)
    else:
        raise ValueError(f"Invalid query type: {args.query_type}")

    prompts = [query_result for _ in range(args.num_prompts)]

    print("prompts", prompts)
    omni_outputs = omni_llm.generate(prompts, sampling_params_list)

    output_dir = args.output_dir if getattr(args, "output_dir", None) else args.output_wav
    if args.query_type is not None:
        output_dir = os.path.join(output_dir, args.query_type)
    os.makedirs(output_dir, exist_ok=True)

    for stage_outputs in omni_outputs:
        if stage_outputs.final_output_type == "text":
            for output in stage_outputs.request_output:
                request_id = output.request_id
                text_output = output.outputs[0].text
                # Save aligned text file per request
                prompt_text = output.prompt
                out_txt = os.path.join(output_dir, f"{request_id}.txt")
                lines = []
                lines.append("Prompt:\n")
                lines.append(str(prompt_text) + "\n")
                lines.append("vllm_text_output:\n")
                lines.append(str(text_output).strip() + "\n")
                try:
                    with open(out_txt, "w", encoding="utf-8") as f:
                        print("lines", lines)
                        f.writelines(lines)
                except Exception as e:
                    print(f"[Warn] Failed writing text file {out_txt}: {e}")
                print(f"Request ID: {request_id}, Text saved to {out_txt}\n")
        elif stage_outputs.final_output_type == "audio":
            for output in stage_outputs.request_output:
                request_id = output.request_id
                audio_tensor = output.multimodal_output.get("audio")

                if audio_tensor is None:
                    continue

                output_wav = os.path.join(output_dir, f"{request_id}.wav")

                # Convert to numpy array and ensure correct format
                audio_numpy = audio_tensor.float().detach().cpu().numpy()

                # Ensure audio is 1D (flatten if needed)
                if audio_numpy.ndim > 1:
                    audio_numpy = audio_numpy.flatten()

                # Save audio file with explicit WAV format
                sf.write(output_wav, audio_numpy, samplerate=24000, format="WAV")
                print(f"Request ID: {request_id}, Audio saved to {output_wav}")


def parse_args():
    parser = FlexibleArgumentParser(description="Demo on using vLLM for offline inference with audio language models")
    parser.add_argument(
        "--model-name",
        "-m",
        type=str,
        default="XiaomiMiMo/MiMo-Audio-7B-Instruct",
        help="Backbone LLM path.",
    )
    parser.add_argument(
        "--text",
        "-t",
        type=str,
        default="ä»Šå¤©å¤©æ°”çœŸå¥½",
        help="input text",
    )
    parser.add_argument(
        "--query-type",
        "-q",
        type=str,
        default="tts_sft",
        choices=query_map.keys(),
        help="Query type.",
    )
    parser.add_argument(
        "--audio-path",
        "-a",
        type=str,
        default=None,
        help="Path to local audio file. If not provided, uses default audio asset.",
    )
    parser.add_argument(
        "--instruct",
        type=str,
        default=None,
        help="instruct",
    )
    parser.add_argument(
        "--enable-stats",
        action="store_true",
        default=True,
        help="Enable writing detailed statistics (default: disabled)",
    )
    parser.add_argument(
        "--init-sleep-seconds",
        type=int,
        default=20,
        help="Sleep seconds after starting each stage process to allow initialization (default: 20)",
    )
    parser.add_argument(
        "--batch-timeout",
        type=int,
        default=5,
        help="Timeout for batching in seconds (default: 5)",
    )
    parser.add_argument(
        "--init-timeout",
        type=int,
        default=5000,
        help="Timeout for initializing stages in seconds (default: 300)",
    )
    parser.add_argument(
        "--shm-threshold-bytes",
        type=int,
        default=65536,
        help="Threshold for using shared memory in bytes (default: 65536)",
    )
    parser.add_argument(
        "--output-dir",
        default="./output_audio",
        help="Output audio wav directory.",
    )
    parser.add_argument(
        "--num-prompts",
        type=int,
        default=1,
        help="Number of prompts to generate.",
    )

    parser.add_argument(
        "--sampling-rate",
        type=int,
        default=24000,
        help="Sampling rate for audio.",
    )
    parser.add_argument(
        "--stage-configs-path",
        type=str,
        default="../../../model_executor/stage_configs/mimo_audio.yaml",
        help="Path to a stage configs file.",
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    main(args)
